# Training Session Scorer Formula
#
# Score training data quality by linking sessions to bead outcomes.
# Run after a convoy completes or on a schedule to keep scores fresh.
#
# Usage:
#   gt formula run training-score-sessions
#   gt formula run training-score-sessions --var scope="convoy:cv-abc123"
#   gt formula run training-score-sessions --var scope="role:polecat"

description = """
Score training session quality using OpenTelemetry data, bead outcomes,
and event history.

Primary scoring source: OTel RecordDone exit_type (COMPLETED/ESCALATED/DEFERRED)
exported to VictoriaMetrics/VictoriaLogs via GT_OTEL_METRICS_URL/GT_OTEL_LOGS_URL.

Fallback chain: OTel → Beads lifecycle → Events trail → Heuristic analysis.

Links session transcripts to the beads they worked on, then scores each
session based on outcome signals: exit type, clean close, rework count,
escalation frequency, time-to-completion vs median.

Run this:
- After a convoy completes (scores all sessions from that convoy)
- On a schedule (weekly rescore of recent sessions)
- Before retraining (ensure scores are fresh)

Output: updated quality scores in the training dataset, scoring report.
"""
formula = "training-score-sessions"
type = "workflow"
version = 1

[[steps]]
id = "discover"
title = "Discover sessions to score"
description = """
Identify which sessions need scoring based on {{scope}}.

**Scope options:**
- `all` — rescore everything
- `convoy:<id>` — sessions from a specific convoy
- `role:<name>` — all sessions for a role (mayor, deacon, etc.)
- `recent:<days>` — sessions from the last N days
- `unscored` — only sessions without outcome scores

**Process:**
1. Query beads database for relevant beads: `bd query "state=closed OR state=superseded"`
2. Cross-reference with events to find associated session IDs
3. Load session metadata from training dataset
4. Report: N sessions found, N already scored, N to score

**Output:** List of (session_id, bead_id) pairs to score.
"""
acceptance = "Session list generated with bead associations"

[[steps]]
id = "score"
title = "Score sessions against bead outcomes"
needs = ["discover"]
description = """
Score each session using OTel telemetry data (primary) with bead lifecycle fallback.

**Data source priority:**
1. OTel `RecordDone` exit_type via VictoriaLogs LogsQL query (most reliable)
2. OTel metrics: `gastown.done.total`, `gastown.sling.dispatches.total`,
   `gastown.daemon.agent_restarts.total` via VictoriaMetrics PromQL
3. Beads lifecycle: `bd show`, `bd history` (fallback when OTel unavailable)
4. Events trail: `~/.gt/.events.jsonl` escalation/rework signals
5. Heuristic: session content analysis (last resort)

**Scoring rubric (0.0 to 1.0):**

| Signal | Weight | Source | Score |
|--------|--------|--------|-------|
| Exit type: COMPLETED | 0.3 | OTel RecordDone / bead state | 1.0=completed, 0.5=escalated, 0.3=deferred |
| Bead closed on first attempt | 0.2 | OTel sling count / bead reopens | 1.0 if yes, 0.0 if no |
| No escalations required | 0.15 | OTel mail ops / events | 1.0 if none, 0.5 per escalation |
| Time within 2x role median | 0.15 | OTel session start/stop | 1.0 if within, scales down |
| Clean PR (merged without rework) | 0.1 | Beads / git history | 1.0 if clean, 0.3 if reworked |
| No error loops (same cmd 3x+) | 0.05 | OTel pane.output / transcript | 1.0 if clean, 0.0 if looping |
| No agent crashes | 0.05 | OTel daemon.restart | 1.0 if none, 0.0 if crashed |

**Process:**
1. Check if OTel data is available (VictoriaMetrics/VictoriaLogs reachable)
2. For each (session_id, bead_id) pair:
   - Query OTel logs for `done` event with matching session_id → get exit_type
   - Query OTel metrics for sling dispatch count, restart count
   - Fall back to `bd show <bead_id>` / `bd history <bead_id>` if OTel unavailable
   - Check events for escalation mails
   - Check session transcript for error loops
3. Compute weighted outcome_score
4. Flag sessions where OTel and beads signals disagree (needs review)
5. Write scores to a scoring report JSONL

**Output:** `output/scores/session_scores.jsonl` with session_id, bead_id, outcome_score, breakdown.
"""
acceptance = "All sessions scored, scoring report written"

[[steps]]
id = "apply"
title = "Apply scores to training dataset"
needs = ["score"]
description = """
Update the training dataset with outcome scores.

**Process:**
1. Load session scores from scoring report
2. Load existing training dataset (gastown_train.jsonl + per-role files)
3. For each sample, update metadata.outcome_score from scoring report
4. Optionally filter: remove samples below threshold (default: keep all, add score only)
5. Regenerate per-role dataset files with updated scores
6. Report: score distribution, samples above/below threshold, role breakdown

**Filtering policy** (set via {{filter_policy}}):
- `score_only` — add scores but keep all samples (default)
- `exclude_below:0.3` — remove samples with outcome_score < 0.3
- `downweight` — keep all but adjust quality_score = quality_score * outcome_score

**Output:** Updated dataset files, filtering report.
"""
acceptance = "Training dataset updated with outcome scores, report generated"

[[steps]]
id = "report"
title = "Generate scoring summary"
needs = ["apply"]
description = """
Generate a human-readable summary of scoring results.

**Report contents:**
- Total sessions scored, score distribution (histogram)
- Per-role breakdown: mean score, worst sessions, best sessions
- Flagged sessions: outcome_score < 0.3 (candidates for exclusion or DPO)
- Recommendations: retrain needed? data quality improving or degrading?
- Comparison to previous scoring run (if available)

**Output:** `sf_workflows/reports/scoring-<date>.md`
"""
acceptance = "Scoring summary written with recommendations"

[vars]
[vars.scope]
description = "What to score: all, convoy:<id>, role:<name>, recent:<days>, unscored"
default = "unscored"

[vars.filter_policy]
description = "How to handle low scores: score_only, exclude_below:<threshold>, downweight"
default = "score_only"
