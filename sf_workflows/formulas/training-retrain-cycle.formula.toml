# Training Retrain Cycle Formula
#
# Full retraining cycle: regenerate data, score, curate, train, evaluate.
# Run when enough new data has accumulated or scores have shifted.
#
# Usage:
#   gt formula run training-retrain-cycle --var version="v2"
#   gt formula run training-retrain-cycle --var version="v2" --var roles="mayor,deacon"

description = """
Full retraining cycle for Gas Town LoRA adapters.

Runs the complete loop: regenerate training data from all sessions,
apply outcome scores, optionally curate with agents, train per-role
adapters, evaluate against baselines, and report.

Run when:
- Dataset has grown 10%+ since last training
- Outcome scores indicate quality shift
- New roles or capabilities added
- After major Gas Town infrastructure changes

This is the top-level formula that composes the other training formulas.
"""
formula = "training-retrain-cycle"
type = "workflow"
version = 1

[[steps]]
id = "extract-sessions"
title = "Extract sessions"
description = """"
Run the data extraction pipeline to get raw session data.

```bash
make -C data extract
```

**Output:** Raw session data in data/extracted/
"""
acceptance = "Sessions extracted successfully"
[[steps]]
id = "score-sessions"
title = "Score sessions"
needs = ["extract-sessions"]
expansion = "training-score-sessions"
description = """"
Invoke training-score-sessions as an expansion to score the extracted sessions.

This embeds the B.1 formula directly into this workflow at step 2.

**Output:** Sessions scored with outcome scores applied to training datasets.
"""
acceptance = "All sessions scored, score report generated"

[[steps]]
id = "apply-curation-policy"
title = "Apply curation policy"
needs = ["score-sessions"]
description = """"
Apply curation policy based on session scores.

**Policy options (set via {{score_threshold}}):**
- Keep samples with outcome_score >= {{score_threshold}}
- Exclude samples below threshold
- Downweight samples based on score

**Output:** Curated datasets ready for transformation.
"""
acceptance = "Curation policy applied, curated datasets ready"

[[steps]]
id = "format-for-axolotl"
title = "Format for Axolotl"
needs = ["apply-curation-policy"]
description = """"
Transform curated data into Axolotl-compatible format.

```bash
make -C data transform
```

**Output:** Formatted datasets in data/transformed/
"""
acceptance = "Data transformed to Axolotl format successfully"

[[steps]]
id = "validate-data"
title = "Validate"
needs = ["format-for-axolotl"]
description = """"
Validate the transformed training data for quality and consistency.

```bash
make -C data validate
```

**Output:** Validation report confirming data quality.
"""
acceptance = "Data validation passed, validation report generated"

[[steps]]
id = "train-adapters"
title = "Train per-role adapter"
needs = ["validate-data"]
description = """"
Train LoRA adapters for each specified role using Axolotl.

For each role in {{roles}}:
1. Verify sufficient data after curation
2. Train adapter: `axolotl train configs/roles/{role}.yml`
3. Monitor training metrics

**Expected time:** ~30-60 min per role on A100.

**Output:** Adapters in {{adapter_output_path}}/
"""
acceptance = "All role adapters trained successfully"


[[steps]]
id = "evaluate-adapters"
title = "Eval against baseline"
needs = ["train-adapters"]
description = """"
Evaluate trained adapters against baseline using role_bench.

For each trained adapter:
1. Load base model + adapter
2. Run role_bench with role-specific scenarios
3. Compare against base model and previous versions

**Output:** Evaluation report with comparison metrics.
"""
acceptance = "All adapters evaluated, comparison report generated"

[[steps]]
id = "report-results"
title = "Report results"
needs = ["evaluate-adapters"]
description = """"
Generate comprehensive retraining cycle report.

**Report sections:**
1. Data summary: samples per role, new data since last train
2. Scoring summary: outcome score distribution, curation decisions
3. Training summary: per-role training metrics
4. Evaluation summary: per-role scores, improvement vs baseline
5. Recommendation: deploy? retrain? collect more data?

**Output:** `sf_workflows/reports/retrain-{{role}}-<date>.md`
"""
acceptance = "Report written with comprehensive results and recommendations"
[vars]
[vars.role]
description = "Role to train adapter for"
default = "polecat"

[vars.gpu_budget]
description = "GPU budget allocation for training"
default = "1"

[vars.score_threshold]
description = "Minimum score threshold for curation"
default = "0.3"

[vars.adapter_output_path]
description = "Path where trained adapters should be saved"
default = "output/adapters"